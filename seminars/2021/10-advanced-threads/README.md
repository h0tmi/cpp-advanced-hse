# Продвинутые потоки

### Futex
TODO

### Lock-free

Говорим про алгоритмы, не использующие блокировок.
* Lock-free не обязательно значит wait-free, мы можем жечь CPU.
* Динамическое выделение памяти не lock-free, но им часто всё равно пользуются.

#### Основные идеи
* Используем атомарные переменные.
* Простые атомарные переменные (int, T*) защищают неатомарную память.
* Активно используем CAS.

#### Compare and swap
Вот такая конструкция позволяет нам делать сложные изменения атомарно, а также "брать лок" на какое-то состояние.
```cpp
old_value = value.load();
while (!value.compare_exchange_weak(old_value, f(old_value))) {
    // old_value might change, as might f(old_value)
}
```
**Это ест CPU.** Операции compare_exchange_[strong/weak] одни из самых дорогих с атомиками.

#### Lock-free stack, single consumer
Пишем простой лок-фри стек: [01-sc-stack.cpp](01-lock-free/01-sc-stack.cpp).

Говорим о том, какие проблемы возникают, когда несколько потоков начинают делать Pop:
* У потоков могут оставаться локальные копии ноды, которая уже удалена в другом потоке.
* ABA: Pop-Push может сохранять адрес верхней ноды стека, если аллокатор решил переиспользовать память.

Детальнее в комментариях к коду.

#### Lock-free stack, multiple consumers
Проблемы у нас именно с преждевременным освобождением памяти.
Обсудим несколько подходов к их решению.
Ниже какой-то общий план, на самом деле тонкостей сильно больше.

##### Hazard pointers
Каждый тред умеет защищать некоторое количество указателей от удаления, помечая их как опасные (hazardous).
В lock-free stack хватает одного указателя на каждый тред.

Особенные действия нужны в Pop:
1. Читаем head и сохраняем его в свой hazard_ptr.
    * После сохранения надо проверить, что head не поменялся, иначе указатель могли уже удалить.
2. Крутимся в цикле, пока не сможем сделать успешный `CAS(head, head->next)`.
3. Обнуляем свой hazard_ptr.
4. Добавляем head в список to_free_list. Он тоже lock-free!
5. Если размер to_free_list достаточно большой, вызываем процедуру его освобождения.

Освобождение to_free_list:
1. Единолично забираем голову to_free_list, делая `exchange(nullptr)`.
2. Проходимся по списку и удаляем указатели, которые мы не нашли как hazard_ptr ни в одном потоке.
3. Оставшиеся кладем обратно в очередь.

**Важный вопрос**: почему у нас нет таких же проблем в to_free_list, как и в самой задаче?
*Потому что один поток атомарно забирает себе владение всем списком сразу.*

Потоки можно идентифицировать по их id, hazard_ptr это просто атомарный указатель на ноду, никакие дополнительные локи здесь не нужны.

**Важно**: для каждого потока мы можем хранить только K указателей.

##### Atomic shared ptr
* Сложный и медленнее, чем всё остальное.
* Есть в C++ 20, но пока не имплементирован.
* Использует CAS на 16 байт.

##### RCU
Эта идея не подходит нам в lock-free stack, так как здесь может быть много конкуррентных писателей.
Но хорошо подходит для сценария, который обсуждали в примере с shared_ptr в позапрошлом семинаре.

Там мы имеем дело с shared resource:
* Хотим быстрые чтения, возможно устаревшей версии ресурса.
* Обновления происходят редко.

Если обновления происходят в один поток (или мы готовы блокироваться), то RCU помогает решить проблему, возникающую со владением:
когда мы обновляем ресурс, старый ресурс нельзя освободить, так как его локальные копии всё ещё могут использоваться в читателях.

**Концептуально:** добавляем счетчики ссылок, которые дешево изменять читателям (wait-free). Обнуления этих счетчиков дожидается писатель, и только потом освобождает память.

Чуть более подробно:
* Писателем поддерживается атомарный монотонно возрастающий счетчик поколений (generation).
* Он увеличивается когда писателю вздумается, это влияет только на количество накапливаемого мусора.
* На каждое поколение храним garbage_queue.
* Писатель кладет старые ресурсы в очередь для соответствующего поколения.
* На каждое поколение есть счетчик активных читателей.
* Иногда писатель ждет, пока счетчики читателей **прошедших** поколений станут равными нулю, и только тогда освобождает соответствующие garbage queue.

Из плюсов такого подхода то, что быть читателем очень дешево (надо сделать парочку atomic read/write), это даже wait-free.
Из минусов как минимум то, что писатель может быть только один. Но с редкими обновлениями обычно не жалко блокироваться для их синхронизации, поэтому на выходе идея очень практична.

Бывают и другие имплементации.


### Future / Promise
Рано или поздно вам захочется передавать данные между потоками.
Например, запустить задачу в потоке, а по завершению задачи выполнить какой-то код.
Известных нам примитивов недостаточно для удобной разработки; на помощь приходят
`Future / Promise`. Это некоторый канал для передачи значения: с одной стороны (Promise) значение
можно записать, с другой (Future) --- дождаться / подписаться. Оказывается довольно удобно,
особенно если есть богатые возможности для комбинирования фьючей.
К сожалению, стандартные фьючи не такие, их никто не использует.

Предлагается написать кода из executors: Future / Promise, Subscribe, пачку комбинаторов (Join, WaitAny). Не слишком шаблонно, 100 -- 200 строк.

https://github.com/facebook/folly/blob/main/folly/docs/Futures.md

### Nanofibers (опционально)
_NB: основная цель здесь -- дать прочувствовать семантику файберов, почему и когда они лучше, чем потоки, а почему и когда хуже; нужно больше слов, в код смотреть скорее не надо даже._

Попробуем посмотреть на максимально простую библиотеку _файберов_ (aka goroutines, green threads, иногда корутины) --- легких потоков в пространстве пользователя с кооперативной многозадачностью.
На файберах работают современные модные асинхронные фреймворки и языки; в первую очередь Go, где
весь язык построен вокруг них.

Что значит страшное определение выше?
1. В пространстве пользователя: выбор потока для запуска и переключение между потоками происходит
без переключения в ядро; переключение в ядро --- дорогая операция.
1. В одно и то же время может выполняться не больше потоков, чем у вас есть ядер в процессоре.
Как тогда работает система? Она очень часто (типичные цифры --- единицы миллисекунд)
переключает потоки выполнения, при этом может создаться
иллюзия, что все работает одновременно. Как следствие, в уже относительно привычном
вам многопоточном программировании система могла снять ваш поток с выполнения в любой момент.
При этом даже понять, что это произошло, довольно сложно.
Такой механизм называется _вытесняющая многозадачность_.
Мы же реализуем _кооперативную многозадачность_: здесь потоки не пытаются бороться за ресурс процессора, а отдают добровольно выполнение другим потокам, когда это нужно. Например, когда файбер блокирует
мьютекс, он отдает выполнение другим файберам.
1. Файберы легкие: создание --- единицы микросекунд, переключение между потоками --- пара десятков
инструкций. Это позволяет эффективно управлять большим количество файберов (сотни тысяч), и
создавать файберы на "каждый чих": например, на каждый запрос в сервер.

Для простоты сегодня nanofibers будут полностью однопоточными; более реалистичные файберы вы увдите
у Ромы Липовского. Покажем, как использовать файберы:
```cpp
nanofibers::Scheduler scheduler;

// Run main routine
scheduler.Run([&] {
    nanofibers::Spawn([] {
        for (int i = 0; i < 3; ++i) {
            printf("{Fiber #1} Ping\n");
            scheduler.Yield();
        }
    });
    nanofibers::Spawn([] {
        for (int i = 0; i < 3; ++i) {
            printf("{Fiber #2} Pong\n");
            scheduler.Yield();
        }
    });
});

// Output:
// {Fiber #1} Ping
// {Fiber #2} Pong
// {Fiber #1} Ping
// {Fiber #2} Pong
// {Fiber #1} Ping
// {Fiber #2} Pong
```

##### АКОС на максималках (для самых любознательных)

Как это работает? Ключевое место --- использование `Yield()` в коде.
`Yield()` отпускает выполнение текущего файбера и переключается на следующий готовый.

##### Захват и переключение контекста
Что есть состояние потока? Это, на самом деле, состояние процессора и стек потока; остальные
данные для него. Состояние процессора в нашем случае почти полностью представимо просто набором
general purpose регистров, так что его можно целиком сохранить в понятную структуру.

Вся магия по переключению между файберами происходит в [context.S](01-nanofibers-simple/context.S).
Данные операции не выразимы через C / C++, но ассемблер совсем простой.
Основная операция --- переключение между двумя контекстами:
```cpp
Context from, to;
if (SaveContext(&from)) {
    JumpContext(&to);
} else {
    // continue
}
```
Семантика хитрая: SaveContext возвращает управление два раза. Первый раз --- сразу после вызова,
а второй --- когда на `from` позовут JumpContext. Например, для такого кода
```cpp
Context ctx;
if (SaveContext(&ctx)) {
    printf("First");
    JumpContext(&ctx);
} else {
    printf("Second");
}
```
Вывод будет
```
First
Second
```

##### Стек
Осталось разобраться со стеками.
На самом деле, стек --- это просто регион памяти, их можно выделять руками.
Стеки выделяются на каждый файбер; это место можно оптимизировать
(использованные стеки класть в кеш). Сделаем пока просто [stack.h](01-nanofibers-simple/stack.h). 

##### Запуск файбера
Чтоб запустить файбер, нужно настроить контекст. rsp укажем на вершину стека; а rip --- instruction pointer --- на точку входа `Fiber::Trampoline`.
Файбер будет выполнять `std::function<void()>`; функтор стейтфул, поэтому нужно дополнительно
аккуратно пробросить этот стейт; сделаем это через глобальную переменную current_fiber.

##### Шедулер
Поддерживаем очередь (intrusive list) готовых к выполнению файберов.
Все время достаем новый файбер, переключаемся из главного контекста (тот поток, что запустил шедулер)
в поток файбера. `Yield()` переключает нас обратно из контекста файбера в контекст шедулера.
Код получается простой: [scheduler.h](01-nanofibers-simple/scheduler.h)

##### Mutex
Мьютекс и ConditionVariable нельзя использовать стандартные: два вызова `mutex.lock()`
заблокируют целиком поток, а мы хотим явно не этого. Пишем свои: вызов `lock()`, который блокируется,
кладет файбер в специфичную для этого мьютекса очередь ожидания [mutex.h](02-nanofibers-mutex/mutex.h).
